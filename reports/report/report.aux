\relax 
\@writefile{toc}{\contentsline {section}{\numberline {1}Legal Disclaimer}{2}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Acknowledgements}{5}}
\citation{control-point}
\citation{control-point}
\citation{control-point}
\citation{control-point}
\@writefile{toc}{\contentsline {section}{\numberline {3}Introduction}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Explaining the Problem}{6}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Code Coverage for Request Server\relax }}{6}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{f1}{{1}{6}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces soil contamination risk, water contamination risk, no risk\relax }}{6}}
\newlabel{f2}{{1}{6}}
\citation{MIML}
\citation{MIML}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Formalising the problem}{7}}
\@writefile{toc}{\contentsline {paragraph}{Independent Binary Classifiers}{7}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Challenges specific to the Pipe Weld Classification Task}{7}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Data Overview}{7}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Count of Redbox images with given label\relax }}{8}}
\newlabel{f3}{{2}{8}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Semantic Complexity}{8}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces The clamp wraps around under the pipe - the glint of a metal rod gives it away\relax }}{8}}
\newlabel{f4}{{2}{8}}
\citation{office}
\citation{surf}
\citation{krizhevsky}
\citation{transfer-learning}
\newlabel{fig:2a}{{3(a)}{9}}
\newlabel{sub@fig:2a}{{(a)}{9}}
\newlabel{fig:2b}{{3(b)}{9}}
\newlabel{sub@fig:2b}{{(b)}{9}}
\newlabel{f5}{{\caption@xref {f5}{ on input line 249}}{9}}
\newlabel{fig:2a}{{4(a)}{9}}
\newlabel{sub@fig:2a}{{(a)}{9}}
\newlabel{fig:2b}{{4(b)}{9}}
\newlabel{sub@fig:2b}{{(b)}{9}}
\newlabel{f6}{{\caption@xref {f6}{ on input line 275}}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Domain Change}{9}}
\citation{krizhevsky}
\citation{transfer-learning}
\citation{decaf}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4}Small Dataset Size}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.5}Class Imbalance}{10}}
\citation{univ-approx}
\@writefile{toc}{\contentsline {section}{\numberline {4}Literature Review}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Supervised Learning}{11}}
\newlabel{f7}{{1}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Approximation vs Generalisation}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}Models of Neurons}{11}}
\@writefile{toc}{\contentsline {paragraph}{Multipolar Biological Neuron}{11}}
\citation{DL-book}
\citation{krizhevsky}
\citation{rectifier}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces a multipolar biological neuron\relax }}{12}}
\newlabel{f8}{{5}{12}}
\@writefile{toc}{\contentsline {paragraph}{Binary Threshold Neuron}{12}}
\@writefile{toc}{\contentsline {paragraph}{Logistic Sigmoid Neuron}{12}}
\newlabel{sigmoid neuron}{{3}{12}}
\@writefile{toc}{\contentsline {paragraph}{Rectified Linear Neuron}{12}}
\newlabel{relu}{{4}{12}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces single-input logistic sigmoid neuron\relax }}{13}}
\newlabel{f9}{{6}{13}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces single-input rectified linear neuron\relax }}{13}}
\newlabel{f10}{{7}{13}}
\@writefile{toc}{\contentsline {paragraph}{Softmax Neuron}{13}}
\citation{DL-book}
\citation{DL-book}
\citation{DL-book}
\citation{Russel&Norvig}
\citation{DL-book}
\citation{MLP-univ-approx}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}Feed-forward Architecture}{14}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces graphical representation of $y = x*sin(a*x+b)$ (source: Bengio 2009)\relax }}{14}}
\newlabel{f11}{{8}{14}}
\@writefile{toc}{\contentsline {paragraph}{Shallow Feed-Forward Neural Networks: the Perceptron}{14}}
\@writefile{toc}{\contentsline {paragraph}{Deep Feed-Forward Neural Networks: the Multilayer Perceptron}{14}}
\citation{goodfellow_street_view}
\citation{DL-book}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces Multi Layer Perceptron with 2 hidden layers\relax }}{15}}
\newlabel{f12}{{9}{15}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.5}Justifying Depth}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces LeNet7 architecture: each square is a kernel\relax }}{15}}
\newlabel{f13}{{10}{15}}
\citation{DL-book}
\citation{Bengio_G+}
\citation{Bengio_G+}
\citation{DL-book}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.6}Backpropagation}{17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.6.1}Compute Error-Weight Partial Derivatives}{17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.6.2}Update Weight Values with Gradient Descent}{17}}
\newlabel{eqn:learning_rule}{{8}{17}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.6.3}Stochastic Gradient Descent}{17}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.7}Overfit}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces an error surface with poor local minima\relax }}{18}}
\newlabel{f14}{{11}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces Overfit as polynomial order increases (source: Bishop 2010)\relax }}{18}}
\newlabel{f15}{{12}{18}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.7.1}Cross Validation}{18}}
\citation{ML-book}
\citation{data-aug}
\citation{dropout}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.7.2}Data Augmentation}{19}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.7.3}Dropout}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces source: (Hinton et al 2012)\relax }}{20}}
\newlabel{f16}{{13}{20}}
\citation{rectifier}
\citation{goodfellow_street_view}
\citation{decaf}
\citation{fergus_tutorial}
\citation{colah}
\citation{zeiler_fergus}
\citation{transfer-learning}
\citation{caffe-website}
\citation{fergus_tutorial}
\citation{SIFT}
\citation{colah}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.8}Deep Convolutional Neural Networks}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces LeNet7 architecture: each square is a kernel\relax }}{21}}
\newlabel{f17}{{14}{21}}
\citation{SIFT}
\citation{zeiler_fergus}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Architecture of CNN from (Krizhevsky et al 2012)\relax }}{22}}
\newlabel{f18}{{15}{22}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.8.1}Pixel Feature}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces Selecting a pixel window and applying a kernel to it (source: convmatrix Gimp documentation)\relax }}{22}}
\newlabel{f19}{{16}{22}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces Producing a kernel map (source: River Trail documentation)\relax }}{23}}
\newlabel{f20}{{17}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces Image, kernel and resulting kernel map (source: convmatrix Gip documentation\relax }}{23}}
\newlabel{f21}{{18}{23}}
\newlabel{eqn:conv_learning_rule}{{9}{23}}
\newlabel{fig:2a}{{19(a)}{23}}
\newlabel{sub@fig:2a}{{(a)}{23}}
\newlabel{fig:2b}{{19(b)}{23}}
\newlabel{sub@fig:2b}{{(b)}{23}}
\newlabel{f22}{{\caption@xref {f22}{ on input line 596}}{23}}
\citation{zeiler_fergus}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.8.2}Non-linear Activation}{24}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.8.3}Pooling aka Spatial Feature}{24}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.8.4}Contrast Normalisation}{24}}
\citation{DL-book}
\citation{rectifier}
\citation{labex-bezout}
\newlabel{fig:2b}{{20(a)}{25}}
\newlabel{sub@fig:2b}{{(a)}{25}}
\newlabel{fig:2a}{{20(b)}{25}}
\newlabel{sub@fig:2a}{{(b)}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces source: (Zeiler and Fergus 2013)\relax }}{25}}
\newlabel{f23}{{\caption@xref {f23}{ on input line 637}}{25}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.9}Local vs Global Optimisation}{25}}
\citation{microsoft-book}
\citation{transfer-learning}
\citation{off-the-shelf}
\citation{ML-book}
\citation{ML-book}
\citation{imbalance}
\citation{maloof}
\citation{zhou}
\citation{f-measure}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.10}Transfer Learning}{26}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {4.10.1}Linear Support Vector Machines}{26}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.11}Class Imbalance}{26}}
\@writefile{toc}{\contentsline {paragraph}{Definition}{26}}
\citation{zhou}
\citation{f-measure}
\citation{coursera}
\citation{ML-book}
\citation{krizhevsky}
\citation{rectifier}
\newlabel{fig:2a}{{21(a)}{28}}
\newlabel{sub@fig:2a}{{(a)}{28}}
\newlabel{fig:2b}{{21(b)}{28}}
\newlabel{sub@fig:2b}{{(b)}{28}}
\newlabel{f24}{{\caption@xref {f24}{ on input line 725}}{28}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Analysis 1: ReLU Activation}{28}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Motivations}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces ReLU path selection (source: Glorot et al 2013)\relax }}{29}}
\newlabel{f25}{{22}{29}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Mathematical Analysis}{29}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}How the Gradient Propagates}{29}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}An Example}{29}}
\citation{DL-book}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces $\mathbb  {R}^3 \rightarrow \mathbb  {R}$ MLP with 1 hidden layer\relax }}{30}}
\newlabel{f26}{{23}{30}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.3}Vanishing Gradient}{30}}
\newlabel{f27}{{\caption@xref {f27}{ on input line 838}}{31}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.4}Impact of the ReLU}{31}}
\citation{rectifier}
\citation{goodfellow_street_view}
\citation{decaf}
\citation{fergus_tutorial}
\citation{colah}
\citation{zeiler_fergus}
\citation{transfer-learning}
\citation{caffe-website}
\citation{soumith-benchmark}
\citation{DL-book}
\@writefile{toc}{\contentsline {section}{\numberline {6}Experiments 1: Simple Clamp Detection}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Motivations}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Implementation: Cuda-Convnet}{33}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Experimentation}{33}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.1}Non-Converging Error Rates}{33}}
\citation{DL-book}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Test Run Training Results\relax }}{34}}
\newlabel{f28}{{25}{34}}
\citation{decaf}
\citation{fergus_tutorial}
\citation{caffe-website}
\citation{DL-book}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.2}Increase Validation Error Precision}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces Test Run with validation errors computed over an unchanging set\relax }}{35}}
\newlabel{f29}{{26}{35}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.3}Periodicity of the training error}{35}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.4}Poor, Sampling-Induced Corner Minima}{35}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces filters learned in a successfully optimised lowest convolutional layer\relax }}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces filters learned at lowest convolutional layer of this network\relax }}{36}}
\newlabel{f30}{{28}{36}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.5}Mislabelling}{37}}
\newlabel{f34}{{\caption@xref {f34}{ on input line 1046}}{37}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces Redbox images with no flags\relax }}{37}}
\newlabel{materialflowChart}{{30}{37}}
\newlabel{f35}{{30}{37}}
\citation{DL-book}
\citation{caffe}
\citation{theano}
\citation{torch7}
\citation{decaf}
\citation{soumith-benchmark}
\@writefile{toc}{\contentsline {section}{\numberline {7}Experiments 2: Transfer Learning}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Motivations}{39}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Implementation}{39}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.1}Caffe}{39}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.2.2}Per Class Accuracy Layer}{39}}
\citation{transfer-learning}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Experimentation}{40}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1}Test Run}{40}}
\@writefile{toc}{\contentsline {paragraph}{Error-accuracy mismatch}{40}}
\newlabel{f36}{{\caption@xref {f36}{ on input line 1126}}{41}}
\@writefile{toc}{\contentsline {paragraph}{Zig-zag}{41}}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces source: Coursera, Neural Networks for Machine Learning\relax }}{41}}
\newlabel{f37}{{32}{41}}
\citation{rectifier}
\citation{transfer-learning}
\newlabel{fig:2a}{{33(a)}{42}}
\newlabel{sub@fig:2a}{{(a)}{42}}
\newlabel{fig:2b}{{33(b)}{42}}
\newlabel{sub@fig:2b}{{(b)}{42}}
\newlabel{f38}{{\caption@xref {f38}{ on input line 1188}}{42}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.2}Freezing Backprop on various layers}{42}}
\citation{transfer-learning}
\citation{decaf}
\citation{labex-bezout}
\citation{transfer-learning}
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces Test Run with validation errors computed over an unchanging set\relax }}{43}}
\newlabel{f29}{{34}{43}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.3}Initialising Free Layers}{43}}
\citation{off-the-shelf}
\citation{ML-book}
\citation{ML-book}
\citation{ML-book}
\newlabel{fig:2a}{{35(a)}{44}}
\newlabel{sub@fig:2a}{{(a)}{44}}
\newlabel{fig:2b}{{35(b)}{44}}
\newlabel{sub@fig:2b}{{(b)}{44}}
\newlabel{f39}{{\caption@xref {f39}{ on input line 1227}}{44}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.4}Softmax vs linear SVM}{44}}
\@writefile{toc}{\contentsline {paragraph}{Analytical Comparison}{44}}
\@writefile{lof}{\contentsline {figure}{\numberline {36}{\ignorespaces Hinge (blue), cross entropy (red), mean squared error (green). source: (Bishop 2010)\relax }}{45}}
\newlabel{f40}{{36}{45}}
\citation{svm-nn}
\newlabel{fig:2a}{{37(a)}{46}}
\newlabel{sub@fig:2a}{{(a)}{46}}
\newlabel{fig:2b}{{37(b)}{46}}
\newlabel{sub@fig:2b}{{(b)}{46}}
\@writefile{lof}{\contentsline {figure}{\numberline {37}{\ignorespaces hinge loss\relax }}{46}}
\newlabel{fig:2}{{37}{46}}
\newlabel{f42}{{\caption@xref {f42}{ on input line 1296}}{46}}
\@writefile{toc}{\contentsline {paragraph}{Empirical Results}{46}}
\citation{f-measure}
\citation{krizhevsky}
\@writefile{toc}{\contentsline {section}{\numberline {8}Experiments 3: Class Imbalance}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Definition}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Motivations}{47}}
\citation{zhou}
\citation{DL-book}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Implementation}{48}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.4}Experimentation}{48}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.1}Test Run}{48}}
\newlabel{fig:2a}{{38(a)}{49}}
\newlabel{sub@fig:2a}{{(a)}{49}}
\newlabel{fig:2b}{{38(b)}{49}}
\newlabel{sub@fig:2b}{{(b)}{49}}
\@writefile{lof}{\contentsline {figure}{\numberline {38}{\ignorespaces no transfer learning\relax }}{49}}
\newlabel{fig:2}{{38}{49}}
\newlabel{f43}{{\caption@xref {f43}{ on input line 1391}}{49}}
\@writefile{lof}{\contentsline {figure}{\numberline {39}{\ignorespaces Clamp Detection, Full Backpropagation\relax }}{49}}
\newlabel{f44}{{39}{49}}
\@writefile{lof}{\contentsline {figure}{\numberline {40}{\ignorespaces Example saddle point in 3D space\relax }}{50}}
\newlabel{f45}{{40}{50}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.2}Transfer Learning}{50}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.3}Hyperparameter Optimisation}{50}}
\@writefile{toc}{\contentsline {paragraph}{Mini-batch Size}{50}}
\newlabel{fig:2a}{{41(a)}{51}}
\newlabel{sub@fig:2a}{{(a)}{51}}
\newlabel{fig:2b}{{41(b)}{51}}
\newlabel{sub@fig:2b}{{(b)}{51}}
\newlabel{fig:2b}{{41(c)}{51}}
\newlabel{sub@fig:2b}{{(c)}{51}}
\@writefile{lof}{\contentsline {figure}{\numberline {41}{\ignorespaces 98\% imbalance, varying levels of transfer learning\relax }}{51}}
\newlabel{fig:2}{{41}{51}}
\newlabel{f46}{{\caption@xref {f46}{ on input line 1446}}{51}}
\newlabel{fig:2a}{{42(a)}{51}}
\newlabel{sub@fig:2a}{{(a)}{51}}
\newlabel{fig:2b}{{42(b)}{51}}
\newlabel{sub@fig:2b}{{(b)}{51}}
\@writefile{lof}{\contentsline {figure}{\numberline {42}{\ignorespaces 98\% imbalance, different mini-batch sizes\relax }}{51}}
\newlabel{fig:2}{{42}{51}}
\newlabel{f47}{{\caption@xref {f47}{ on input line 1500}}{51}}
\citation{zhou}
\citation{maloof}
\citation{zhou}
\citation{maloof}
\citation{maloof}
\citation{zhou}
\@writefile{toc}{\contentsline {paragraph}{Learning Rate}{52}}
\newlabel{fig:2a}{{43(a)}{52}}
\newlabel{sub@fig:2a}{{(a)}{52}}
\newlabel{fig:2b}{{43(b)}{52}}
\newlabel{sub@fig:2b}{{(b)}{52}}
\@writefile{lof}{\contentsline {figure}{\numberline {43}{\ignorespaces 98\% imbalance, different learning rates\relax }}{52}}
\newlabel{fig:2}{{43}{52}}
\newlabel{f48}{{\caption@xref {f48}{ on input line 1531}}{52}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.4}Over-Sampling}{52}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.5}Under-Sampling}{52}}
\newlabel{fig:2a}{{44(a)}{53}}
\newlabel{sub@fig:2a}{{(a)}{53}}
\newlabel{fig:2b}{{44(b)}{53}}
\newlabel{sub@fig:2b}{{(b)}{53}}
\newlabel{f49}{{\caption@xref {f49}{ on input line 1557}}{53}}
\@writefile{toc}{\contentsline {paragraph}{Under-Sampling algorithm}{53}}
\citation{zhou}
\citation{soumith-benchmark}
\citation{maloof}
\newlabel{fig:2a}{{45(a)}{54}}
\newlabel{sub@fig:2a}{{(a)}{54}}
\newlabel{fig:2b}{{45(b)}{54}}
\newlabel{sub@fig:2b}{{(b)}{54}}
\newlabel{f50}{{\caption@xref {f50}{ on input line 1627}}{54}}
\citation{imbalance}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.6}Threshold-Moving}{55}}
\@writefile{toc}{\contentsline {paragraph}{Motivations}{55}}
\@writefile{toc}{\contentsline {paragraph}{Implementation}{55}}
\@writefile{toc}{\contentsline {paragraph}{Results}{56}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Impact of threshold-moving\relax }}{56}}
\newlabel{f51}{{3}{56}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.7}Bayesian Cross Entropy Cost Function}{56}}
\@writefile{toc}{\contentsline {paragraph}{Motivations}{56}}
\newlabel{fig:2a}{{46(a)}{58}}
\newlabel{sub@fig:2a}{{(a)}{58}}
\newlabel{fig:2b}{{46(b)}{58}}
\newlabel{sub@fig:2b}{{(b)}{58}}
\@writefile{lof}{\contentsline {figure}{\numberline {46}{\ignorespaces Ground Sheet detection, different backpropagation formulae\relax }}{58}}
\newlabel{fig:2}{{46}{58}}
\newlabel{f52}{{\caption@xref {f52}{ on input line 1766}}{58}}
\@writefile{toc}{\contentsline {paragraph}{Implementation}{58}}
\@writefile{toc}{\contentsline {paragraph}{Results}{58}}
\newlabel{fig:2a}{{47(a)}{59}}
\newlabel{sub@fig:2a}{{(a)}{59}}
\newlabel{fig:2b}{{47(b)}{59}}
\newlabel{sub@fig:2b}{{(b)}{59}}
\@writefile{lof}{\contentsline {figure}{\numberline {47}{\ignorespaces 98\% imbalance, different cost functions\relax }}{59}}
\newlabel{fig:2}{{47}{59}}
\newlabel{f53}{{\caption@xref {f53}{ on input line 1791}}{59}}
\@writefile{toc}{\contentsline {paragraph}{Further Research}{60}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {8.4.8}Error-accuracy mismatch revisited}{60}}
\@writefile{toc}{\contentsline {paragraph}{SGD mechanics}{60}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Experiments 4: Conserving Spatial Information}{61}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Motivations}{61}}
\@writefile{lof}{\contentsline {figure}{\numberline {48}{\ignorespaces Heat-map representation of kernel map\relax }}{61}}
\newlabel{f60}{{48}{61}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Implementation}{61}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Experimentation}{61}}
\newlabel{fig:2a}{{49(a)}{62}}
\newlabel{sub@fig:2a}{{(a)}{62}}
\newlabel{fig:2b}{{49(b)}{62}}
\newlabel{sub@fig:2b}{{(b)}{62}}
\@writefile{lof}{\contentsline {figure}{\numberline {49}{\ignorespaces Test Run\relax }}{62}}
\newlabel{fig:2}{{49}{62}}
\newlabel{f54}{{\caption@xref {f54}{ on input line 1884}}{62}}
\newlabel{fig:2a}{{50(a)}{62}}
\newlabel{sub@fig:2a}{{(a)}{62}}
\newlabel{fig:2b}{{50(b)}{62}}
\newlabel{sub@fig:2b}{{(b)}{62}}
\@writefile{lof}{\contentsline {figure}{\numberline {50}{\ignorespaces Remove pooling and a fc layer\relax }}{62}}
\newlabel{fig:2}{{50}{62}}
\newlabel{f55}{{\caption@xref {f55}{ on input line 1909}}{62}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.3.1}Test Run}{62}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.3.2}Remove pooling and a fc layer}{62}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.3.3}Under-Sampling benchmark}{62}}
\newlabel{fig:2a}{{51(a)}{63}}
\newlabel{sub@fig:2a}{{(a)}{63}}
\newlabel{fig:2b}{{51(b)}{63}}
\newlabel{sub@fig:2b}{{(b)}{63}}
\@writefile{lof}{\contentsline {figure}{\numberline {51}{\ignorespaces Under-Sampling\relax }}{63}}
\newlabel{fig:2}{{51}{63}}
\newlabel{f56}{{\caption@xref {f56}{ on input line 1933}}{63}}
\newlabel{fig:2a}{{52(a)}{63}}
\newlabel{sub@fig:2a}{{(a)}{63}}
\newlabel{fig:2b}{{52(b)}{63}}
\newlabel{sub@fig:2b}{{(b)}{63}}
\@writefile{lof}{\contentsline {figure}{\numberline {52}{\ignorespaces Combining Less Pooling and Bayesian Cross Entropy\relax }}{63}}
\newlabel{fig:2}{{52}{63}}
\newlabel{f57}{{\caption@xref {f57}{ on input line 1957}}{63}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.3.4}Remove Pooling, a FC layer and add Bayesian Cross Entropy}{63}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.3.5}Softmax Bayesian Cross Entropy and lower learning rate}{63}}
\citation{soumith-benchmark}
\newlabel{fig:2a}{{53(a)}{64}}
\newlabel{sub@fig:2a}{{(a)}{64}}
\newlabel{fig:2b}{{53(b)}{64}}
\newlabel{sub@fig:2b}{{(b)}{64}}
\@writefile{lof}{\contentsline {figure}{\numberline {53}{\ignorespaces Bayesian Cross Entropy by itself\relax }}{64}}
\newlabel{fig:2}{{53}{64}}
\newlabel{f58}{{\caption@xref {f58}{ on input line 1982}}{64}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.3.6}Reduce gradient noisiness of Bayesian cross entropy}{64}}
\citation{krizhevsky}
\newlabel{fig:2a}{{54(a)}{65}}
\newlabel{sub@fig:2a}{{(a)}{65}}
\newlabel{fig:2b}{{54(b)}{65}}
\newlabel{sub@fig:2b}{{(b)}{65}}
\@writefile{lof}{\contentsline {figure}{\numberline {54}{\ignorespaces Reduce gradient noisiness with mini-batch size\relax }}{65}}
\newlabel{fig:2}{{54}{65}}
\newlabel{f68}{{\caption@xref {f68}{ on input line 2013}}{65}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}Effect of Removing Pooling at conv5}{65}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {9.4.1}Concluding Remarks}{65}}
\@writefile{toc}{\contentsline {section}{\numberline {10}Final Results}{67}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces Final Results\relax }}{67}}
\newlabel{f59}{{4}{67}}
\newlabel{fig:gull}{{55(a)}{68}}
\newlabel{sub@fig:gull}{{(a)}{68}}
\newlabel{fig:tiger}{{55(b)}{68}}
\newlabel{sub@fig:tiger}{{(b)}{68}}
\newlabel{fig:mouse}{{55(c)}{68}}
\newlabel{sub@fig:mouse}{{(c)}{68}}
\newlabel{fig:gull}{{55(d)}{68}}
\newlabel{sub@fig:gull}{{(d)}{68}}
\newlabel{fig:tiger}{{55(e)}{68}}
\newlabel{sub@fig:tiger}{{(e)}{68}}
\newlabel{fig:mouse}{{55(f)}{68}}
\newlabel{sub@fig:mouse}{{(f)}{68}}
\newlabel{fig:gull}{{55(g)}{68}}
\newlabel{sub@fig:gull}{{(g)}{68}}
\newlabel{fig:tiger}{{55(h)}{68}}
\newlabel{sub@fig:tiger}{{(h)}{68}}
\newlabel{fig:mouse}{{55(i)}{68}}
\newlabel{sub@fig:mouse}{{(i)}{68}}
\newlabel{fig:gull}{{55(j)}{68}}
\newlabel{sub@fig:gull}{{(j)}{68}}
\@writefile{lof}{\contentsline {figure}{\numberline {55}{\ignorespaces Training Plots of Best Performing Models\relax }}{68}}
\newlabel{fig:animals}{{55}{68}}
\newlabel{f69}{{55}{68}}
\@writefile{toc}{\contentsline {section}{\numberline {11}Conclusions and Future Work}{69}}
\bibcite{krizhevsky}{1}
\bibcite{rectifier}{2}
\bibcite{SIFT}{3}
\bibcite{goodfellow_street_view}{4}
\bibcite{svm-nn}{5}
\bibcite{ReLU_RBM}{6}
\bibcite{dropout}{7}
\bibcite{data-aug}{8}
\bibcite{decaf}{9}
\bibcite{fergus_tutorial}{10}
\bibcite{off-the-shelf}{11}
\bibcite{colah}{12}
\bibcite{MIML}{13}
\bibcite{control-point}{14}
\bibcite{labex-bezout}{15}
\bibcite{univ-approx}{16}
\bibcite{DL-book}{17}
\bibcite{ML-book}{18}
\bibcite{zeiler_fergus}{19}
\bibcite{Russel & Norvig}{20}
\bibcite{MLP-univ-approx}{21}
\bibcite{office}{22}
\bibcite{Bengio_G+}{23}
\bibcite{surf}{24}
\bibcite{transfer-learning}{25}
\bibcite{cuda-convnet}{26}
\bibcite{microsoft-book}{27}
\bibcite{caffe-website}{28}
\bibcite{nips-tut}{29}
\bibcite{soumith-benchmark}{30}
\bibcite{maloof}{31}
\bibcite{zhou}{32}
\bibcite{f-measure}{33}
\bibcite{imbalance}{34}
\bibcite{coursera}{35}
\bibcite{theano}{36}
\bibcite{torch7}{37}
