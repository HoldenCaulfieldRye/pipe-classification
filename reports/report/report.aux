\relax 
\citation{control-point}
\citation{control-point}
\citation{control-point}
\citation{control-point}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Defining the Problem}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Explaining the Problem}{4}}
\citation{MIML}
\citation{MIML}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Code Coverage for Request Server\relax }}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Formalising the problem: Multi-Instance Multi-Label Supervised Learning}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Supervised Learning}{5}}
\citation{univ-approx}
\newlabel{learning: optimisation equation}{{1}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4}Approximation vs Generalisation}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Architecture of a Deep Convolutional Neural Network with Rectified Linear Neurons}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Models of Neurons}{6}}
\@writefile{toc}{\contentsline {paragraph}{Multipolar Biological Neuron}{6}}
\citation{DL-book}
\citation{krizhevsky}
\citation{rectifier}
\@writefile{toc}{\contentsline {paragraph}{Binary Threshold Neuron}{7}}
\@writefile{toc}{\contentsline {paragraph}{Logistic Sigmoid Neuron}{7}}
\newlabel{sigmoid neuron}{{3}{7}}
\@writefile{toc}{\contentsline {paragraph}{Rectified Linear Neuron}{7}}
\newlabel{relu}{{4}{7}}
\@writefile{toc}{\contentsline {paragraph}{Softmax Neuron}{7}}
\citation{DL-book}
\citation{DL-book}
\citation{DL-book}
\citation{Russel&Norvig}
\citation{DL-book}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Feed-Forward Architecture}{8}}
\@writefile{toc}{\contentsline {paragraph}{Shallow Feed-Forward Neural Networks: the Perceptron}{8}}
\@writefile{toc}{\contentsline {paragraph}{Deep Feed-Forward Neural Networks: the Multilayer Perceptron}{8}}
\citation{MLP-univ-approx}
\@writefile{toc}{\contentsline {paragraph}{Deep Convolutional Neural Networks: for translation invariance}{9}}
\@writefile{toc}{\contentsline {subparagraph}{Kernels}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Topology of Deep Neural Networks}{9}}
\@writefile{toc}{\contentsline {paragraph}{Topology of $tanh$ Layer}{9}}
\@writefile{toc}{\contentsline {subparagraph}{Homeomorphism}{9}}
\citation{nips-tut}
\@writefile{toc}{\contentsline {paragraph}{Topology of ReLU Layer}{10}}
\@writefile{toc}{\contentsline {paragraph}{Ambient Isotopy}{10}}
\@writefile{toc}{\contentsline {paragraph}{The Manifold Hypothesis}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Krizhevsky 2012 explained}{10}}
\@writefile{toc}{\contentsline {paragraph}{Operations in each layer}{10}}
\@writefile{toc}{\contentsline {subparagraph}{Filter aka Pixel Feature}{10}}
\@writefile{toc}{\contentsline {subparagraph}{Non-linearity}{10}}
\@writefile{toc}{\contentsline {subparagraph}{Pooling aka Spatial Feature}{11}}
\@writefile{toc}{\contentsline {subparagraph}{Possible Normalisation}{11}}
\@writefile{toc}{\contentsline {paragraph}{Dropout}{11}}
\@writefile{toc}{\contentsline {paragraph}{Data augmentation}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Training: Backpropagation}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Compute Error-Weight Partial Derivatives}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Update Weight Values (with Gradient Descent)}{11}}
\citation{train&test}
\citation{office}
\citation{surf}
\@writefile{toc}{\contentsline {paragraph}{Training, Validation and Test Sets}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Early Stopping}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Challenges specific to the Pipe Weld Classification Task}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}Data Overview}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}Multi-Tagging}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.3}Domain Change}{12}}
\citation{krizhevsky}
\citation{transfer-learning}
\citation{krizhevsky}
\citation{transfer-learning}
\citation{decaf}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Count of Redbox images with given label\relax }}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.4}Small Dataset Size}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.5}Class Imbalance}{13}}
\citation{f-measure}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces soil contamination risk (left), water contamination risk (centre), no risk (right)\relax }}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces a multipolar biological neuron\relax }}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces single-input logistic sigmoid neuron\relax }}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces single-input rectified linear neuron\relax }}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces graphical representation of $y = x*sin(a*x+b)$ and of a feed-forward neural network\relax }}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces LeNet7 architecture: each square is a kernel\relax }}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Different kinds of ANNs\relax }}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces an error surface with poor local minima\relax }}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces left: a Redbox photo - right: a Bluebox photo\relax }}{18}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Analysis 1: ReLU Activation}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Analysis 2: Early Stopping}{19}}
\@writefile{toc}{\contentsline {paragraph}{Gradient Descent}{19}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Task 1: Generic Clamp Detection}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Motivations}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Implementation: Cuda-Convnet}{20}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Cuda-Convnet: An Out-of-the-box API}{20}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Hardware: NVidia GeForce GTX 780}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Discovery}{20}}
\citation{krizhevsky}
\citation{transfer-learning}
\citation{decaf}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces The clamp wraps around under the pipe - the glint of a metal rod gives it away\relax }}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces This clamp is not a weld clamp\relax }}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces The clamp on the vertical rod is not a weld clamp\relax }}{21}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Non-Converging Error Rates}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces Test Run Training Results\relax }}{22}}
\@writefile{toc}{\contentsline {paragraph}{Increase Test Error Precision}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces test and validation error rates for clamp detection after fixing a large test range\relax }}{23}}
\@writefile{toc}{\contentsline {paragraph}{Alter Momentum}{24}}
\@writefile{toc}{\contentsline {subparagraph}{Increase}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces test and validation error rates for clamp detection after raising momentum\relax }}{24}}
\@writefile{toc}{\contentsline {subparagraph}{Decrease}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces test and validation error rates for clamp detection after setting momentum to zero\relax }}{25}}