\relax 
\citation{control-point}
\citation{control-point}
\citation{control-point}
\citation{control-point}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{4}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Defining the Problem}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.1}Explaining the Problem}{4}}
\citation{MIML}
\citation{MIML}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Code Coverage for Request Server\relax }}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.2}Formalising the problem: Multi-Instance Multi-Label Supervised Learning}{5}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.3}Supervised Learning}{5}}
\citation{univ-approx}
\newlabel{learning: optimisation equation}{{1}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.1.4}Approximation vs Generalisation}{6}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Architecture of a Deep Convolutional Neural Network with Rectified Linear Neurons}{6}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.1}Models of Neurons}{6}}
\@writefile{toc}{\contentsline {paragraph}{Multipolar Biological Neuron}{6}}
\citation{DL-book}
\citation{krizhevsky}
\citation{rectifier}
\@writefile{toc}{\contentsline {paragraph}{Binary Threshold Neuron}{7}}
\@writefile{toc}{\contentsline {paragraph}{Logistic Sigmoid Neuron}{7}}
\newlabel{sigmoid neuron}{{3}{7}}
\@writefile{toc}{\contentsline {paragraph}{Rectified Linear Neuron}{7}}
\newlabel{relu}{{4}{7}}
\@writefile{toc}{\contentsline {paragraph}{Softmax Neuron}{7}}
\citation{DL-book}
\citation{DL-book}
\citation{DL-book}
\citation{Russel&Norvig}
\citation{DL-book}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.2}Feed-Forward Architecture}{8}}
\@writefile{toc}{\contentsline {paragraph}{Shallow Feed-Forward Neural Networks: the Perceptron}{8}}
\@writefile{toc}{\contentsline {paragraph}{Deep Feed-Forward Neural Networks: the Multilayer Perceptron}{8}}
\citation{MLP-univ-approx}
\@writefile{toc}{\contentsline {paragraph}{Deep Convolutional Neural Networks: for translation invariance}{9}}
\@writefile{toc}{\contentsline {subparagraph}{Kernels}{9}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.3}Topology of Deep Neural Networks}{9}}
\@writefile{toc}{\contentsline {paragraph}{Topology of $tanh$ Layer}{9}}
\@writefile{toc}{\contentsline {subparagraph}{Homeomorphism}{9}}
\citation{nips-tut}
\@writefile{toc}{\contentsline {paragraph}{Topology of ReLU Layer}{10}}
\@writefile{toc}{\contentsline {paragraph}{Ambient Isotopy}{10}}
\@writefile{toc}{\contentsline {paragraph}{The Manifold Hypothesis}{10}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.2.4}Krizhevsky 2012 explained}{10}}
\@writefile{toc}{\contentsline {paragraph}{Operations in each layer}{10}}
\@writefile{toc}{\contentsline {subparagraph}{Filter aka Pixel Feature}{10}}
\@writefile{toc}{\contentsline {subparagraph}{Non-linearity}{10}}
\@writefile{toc}{\contentsline {subparagraph}{Pooling aka Spatial Feature}{11}}
\@writefile{toc}{\contentsline {subparagraph}{Possible Normalisation}{11}}
\@writefile{toc}{\contentsline {paragraph}{Dropout}{11}}
\@writefile{toc}{\contentsline {paragraph}{Data augmentation}{11}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Training: Backpropagation}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.1}Compute Error-Weight Partial Derivatives}{11}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.2}Update Weight Values (with Gradient Descent)}{11}}
\citation{train&test}
\citation{office}
\citation{surf}
\@writefile{toc}{\contentsline {paragraph}{Training, Validation and Test Sets}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.3.3}Early Stopping}{12}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Challenges specific to the Pipe Weld Classification Task}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.1}Data Overview}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.2}Multi-Tagging}{12}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.3}Domain Change}{12}}
\citation{krizhevsky}
\citation{transfer-learning}
\citation{krizhevsky}
\citation{transfer-learning}
\citation{decaf}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Count of Redbox images with given label\relax }}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.4}Small Dataset Size}{13}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.4.5}Class Imbalance}{13}}
\citation{f-measure}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces soil contamination risk (left), water contamination risk (centre), no risk (right)\relax }}{15}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces a multipolar biological neuron\relax }}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces single-input logistic sigmoid neuron\relax }}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces single-input rectified linear neuron\relax }}{16}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces graphical representation of $y = x*sin(a*x+b)$ and of a feed-forward neural network\relax }}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces LeNet7 architecture: each square is a kernel\relax }}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {7}{\ignorespaces Different kinds of ANNs\relax }}{17}}
\@writefile{lof}{\contentsline {figure}{\numberline {8}{\ignorespaces an error surface with poor local minima\relax }}{18}}
\@writefile{lof}{\contentsline {figure}{\numberline {9}{\ignorespaces left: a Redbox photo - right: a Bluebox photo\relax }}{18}}
\citation{nair_hinton}
\citation{rectifier}
\citation{univ-approx}
\citation{krizhevsky}
\@writefile{toc}{\contentsline {section}{\numberline {3}Analysis 1: ReLU Activation}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Motivations}{19}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Literature Review}{19}}
\@writefile{lof}{\contentsline {figure}{\numberline {10}{\ignorespaces ReLU path selection\relax }}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Proposed Mathematical Explanation}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Training: Backpropagation}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}How the Gradient Propagates}{20}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}ReLUs Train Faster}{21}}
\@writefile{lof}{\contentsline {figure}{\numberline {11}{\ignorespaces Training AlexNet (Krizhevsky et al 2012)\relax }}{21}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Analysis 2: Early Stopping}{21}}
\@writefile{toc}{\contentsline {paragraph}{Gradient Descent}{22}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Task 1: Generic Clamp Detection}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Motivations}{22}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Implementation: Cuda-Convnet}{22}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.1}Cuda-Convnet: An Out-of-the-box API}{22}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.2.2}Hardware: NVidia GeForce GTX 780}{23}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Discovery}{23}}
\@writefile{lof}{\contentsline {figure}{\numberline {12}{\ignorespaces The clamp wraps around under the pipe - the glint of a metal rod gives it away\relax }}{23}}
\citation{krizhevsky}
\citation{transfer-learning}
\citation{decaf}
\@writefile{lof}{\contentsline {figure}{\numberline {13}{\ignorespaces This clamp is not a weld clamp\relax }}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {14}{\ignorespaces The clamp on the vertical rod is not a weld clamp\relax }}{24}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.1}Non-Converging Error Rates}{24}}
\@writefile{lof}{\contentsline {figure}{\numberline {15}{\ignorespaces Test Run Training Results\relax }}{25}}
\@writefile{toc}{\contentsline {paragraph}{Increase Test Error Precision}{25}}
\@writefile{lof}{\contentsline {figure}{\numberline {16}{\ignorespaces test and validation error rates for clamp detection after fixing a large test range\relax }}{25}}
\@writefile{toc}{\contentsline {paragraph}{Alter Momentum}{26}}
\@writefile{toc}{\contentsline {subparagraph}{Increase}{26}}
\@writefile{toc}{\contentsline {subparagraph}{Decrease}{26}}
\@writefile{lof}{\contentsline {figure}{\numberline {17}{\ignorespaces test and validation error rates for clamp detection after raising momentum\relax }}{27}}
\@writefile{lof}{\contentsline {figure}{\numberline {18}{\ignorespaces test and validation error rates for clamp detection after setting momentum to zero\relax }}{27}}
\@writefile{toc}{\contentsline {paragraph}{Reduce Learning Rate}{27}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.2}Class Imbalance}{27}}
\@writefile{toc}{\contentsline {paragraph}{Stuck in Sampling-Induced, "fake" Corner Minimum}{28}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Class Proportions Across Batches that Score Different Training Errors\relax }}{28}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.3}Mislabelling}{28}}
\@writefile{lof}{\contentsline {figure}{\numberline {19}{\ignorespaces test and validation error rates for clamp detection after setting momentum to zero\relax }}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {20}{\ignorespaces filters learned from a successfully optimised lowest convolutional layer\relax }}{29}}
\@writefile{lof}{\contentsline {figure}{\numberline {21}{\ignorespaces filters learned at lowest convolutional layer of this network\relax }}{29}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.3.4}Data Complexity}{30}}
\citation{caffe-website}
\@writefile{toc}{\contentsline {section}{\numberline {6}Task 2: Transfer Learning}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}Motivations}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Implementation: Caffe}{31}}
\@writefile{toc}{\contentsline {paragraph}{leveldb}{31}}
\@writefile{toc}{\contentsline {paragraph}{Class Imbalance Solver}{31}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.3}Experimentation}{32}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.1}Test Run}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {22}{\ignorespaces Without transfer learning\relax }}{32}}
\@writefile{lof}{\contentsline {figure}{\numberline {23}{\ignorespaces With transfer learning\relax }}{33}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.2}Initialising Free Layers}{33}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.3}Freezing Backprop on various layers}{33}}
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig:2a}{{24(a)}{34}}
\newlabel{sub@fig:2a}{{(a)}{34}}
\newlabel{fig:2b}{{24(b)}{34}}
\newlabel{sub@fig:2b}{{(b)}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {24}{\ignorespaces Another figure\relax }}{34}}
\newlabel{fig:2}{{24}{34}}
\newlabel{fig:2a}{{25(a)}{34}}
\newlabel{sub@fig:2a}{{(a)}{34}}
\newlabel{fig:2b}{{25(b)}{34}}
\newlabel{sub@fig:2b}{{(b)}{34}}
\@writefile{lof}{\contentsline {figure}{\numberline {25}{\ignorespaces Another figure\relax }}{34}}
\newlabel{fig:2}{{25}{34}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {6.3.4}Parametric vs Non-parametric}{35}}
\newlabel{fig:2a}{{26(a)}{36}}
\newlabel{sub@fig:2a}{{(a)}{36}}
\newlabel{fig:2b}{{26(b)}{36}}
\newlabel{sub@fig:2b}{{(b)}{36}}
\@writefile{lof}{\contentsline {figure}{\numberline {26}{\ignorespaces hinge loss\relax }}{36}}
\newlabel{fig:2}{{26}{36}}
\@writefile{toc}{\contentsline {section}{\numberline {7}Task 3: Class Imbalance}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.1}Motivations}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.2}Implementation}{37}}
\@writefile{toc}{\contentsline {subsection}{\numberline {7.3}Experimentation}{37}}
\newlabel{fig:2a}{{27(a)}{38}}
\newlabel{sub@fig:2a}{{(a)}{38}}
\newlabel{fig:2b}{{27(b)}{38}}
\newlabel{sub@fig:2b}{{(b)}{38}}
\@writefile{lof}{\contentsline {figure}{\numberline {27}{\ignorespaces no transfer learning\relax }}{38}}
\newlabel{fig:2}{{27}{38}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.1}Test Run}{38}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.2}Transfer Learning}{38}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.3}Batch Size}{38}}
\newlabel{fig:2a}{{28(a)}{39}}
\newlabel{sub@fig:2a}{{(a)}{39}}
\newlabel{fig:2b}{{28(b)}{39}}
\newlabel{sub@fig:2b}{{(b)}{39}}
\newlabel{fig:2b}{{28(c)}{39}}
\newlabel{sub@fig:2b}{{(c)}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {28}{\ignorespaces 98\% imbalance, varying levels of transfer learning\relax }}{39}}
\newlabel{fig:2}{{28}{39}}
\@writefile{lof}{\contentsline {figure}{\numberline {29}{\ignorespaces Clamp Detection, Full Backpropagation\relax }}{40}}
\newlabel{fig:2a}{{30(a)}{40}}
\newlabel{sub@fig:2a}{{(a)}{40}}
\newlabel{fig:2b}{{30(b)}{40}}
\newlabel{sub@fig:2b}{{(b)}{40}}
\@writefile{lof}{\contentsline {figure}{\numberline {30}{\ignorespaces 98\% imbalance, different mini-batch sizes\relax }}{40}}
\newlabel{fig:2}{{30}{40}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.4}Learning Rate}{40}}
\newlabel{fig:2a}{{31(a)}{41}}
\newlabel{sub@fig:2a}{{(a)}{41}}
\newlabel{fig:2b}{{31(b)}{41}}
\newlabel{sub@fig:2b}{{(b)}{41}}
\@writefile{lof}{\contentsline {figure}{\numberline {31}{\ignorespaces 98\% imbalance, different learning rates\relax }}{41}}
\newlabel{fig:2}{{31}{41}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.5}Bayesian Cross Entropy Cost Function}{41}}
\@writefile{toc}{\contentsline {paragraph}{Motivations}{41}}
\@writefile{toc}{\contentsline {paragraph}{Implementation}{41}}
\newlabel{fig:2a}{{32(a)}{42}}
\newlabel{sub@fig:2a}{{(a)}{42}}
\newlabel{fig:2b}{{32(b)}{42}}
\newlabel{sub@fig:2b}{{(b)}{42}}
\@writefile{lof}{\contentsline {figure}{\numberline {32}{\ignorespaces Ground Sheet detection, different backpropagation formulae\relax }}{42}}
\newlabel{fig:2}{{32}{42}}
\@writefile{toc}{\contentsline {paragraph}{Results}{42}}
\newlabel{fig:2a}{{33(a)}{43}}
\newlabel{sub@fig:2a}{{(a)}{43}}
\newlabel{fig:2b}{{33(b)}{43}}
\newlabel{sub@fig:2b}{{(b)}{43}}
\@writefile{lof}{\contentsline {figure}{\numberline {33}{\ignorespaces 98\% imbalance, different cost functions\relax }}{43}}
\newlabel{fig:2}{{33}{43}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.6}Under-Sampling}{43}}
\@writefile{toc}{\contentsline {paragraph}{Overfitting consequences of under-sampling}{43}}
\@writefile{lof}{\contentsline {figure}{\numberline {34}{\ignorespaces Clamp Detection, Full Backpropagation\relax }}{43}}
\@writefile{lof}{\contentsline {figure}{\numberline {35}{\ignorespaces Clamp Detection, Full Backpropagation\relax }}{44}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.7}Over-Sampling}{44}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {7.3.8}Test-time threshold}{44}}
\@writefile{lof}{\contentsline {figure}{\numberline {36}{\ignorespaces Clamp Detection, Full Backpropagation\relax }}{45}}
\@writefile{lof}{\contentsline {figure}{\numberline {37}{\ignorespaces Clamp Detection, Full Backpropagation\relax }}{45}}
\@writefile{lof}{\contentsline {figure}{\numberline {38}{\ignorespaces Clamp Detection, Full Backpropagation\relax }}{46}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Task 4: Conserving Spatial Information}{46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.1}Motivations}{46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.2}Implementation}{46}}
\@writefile{toc}{\contentsline {subsection}{\numberline {8.3}Experimentation}{46}}
\@writefile{toc}{\contentsline {paragraph}{Test Run}{46}}
\@writefile{toc}{\contentsline {paragraph}{Remove pooling and an fc layer}{47}}
\@writefile{toc}{\contentsline {paragraph}{Softmax Bayesian Cross Entropy}{47}}
\@writefile{toc}{\contentsline {section}{\numberline {9}Final Results}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.1}Merging Classes}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.2}Learning Rate}{47}}
\@writefile{toc}{\contentsline {paragraph}{Step}{47}}
\@writefile{toc}{\contentsline {paragraph}{Exp}{47}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.3}Soil Risk Contamination Task}{48}}
\@writefile{lof}{\contentsline {figure}{\numberline {39}{\ignorespaces Clamp Detection, Full Backpropagation\relax }}{48}}
\@writefile{toc}{\contentsline {paragraph}{Test Run}{48}}
\@writefile{toc}{\contentsline {subparagraph}{Training Results}{49}}
\@writefile{toc}{\contentsline {subparagraph}{Observations}{49}}
\@writefile{toc}{\contentsline {paragraph}{Get more evidence}{49}}
\@writefile{toc}{\contentsline {subsection}{\numberline {9.4}Hatch Markings}{49}}
\@writefile{lof}{\contentsline {figure}{\numberline {40}{\ignorespaces Clamp Detection, Full Backpropagation\relax }}{50}}
\bibcite{decaf}{1}
\bibcite{MIML}{2}
\bibcite{f-measure}{3}
\bibcite{control-point}{4}
\bibcite{univ-approx}{5}
\bibcite{DL-book}{6}
\@writefile{toc}{\contentsline {section}{\numberline {10}Conclusions and Future Work}{51}}
\bibcite{Russel & Norvig}{7}
\bibcite{krizhevsky}{8}
\bibcite{rectifier}{9}
\bibcite{MLP-univ-approx}{10}
\bibcite{office}{11}
\bibcite{surf}{12}
\bibcite{transfer-learning}{13}
\bibcite{f-measure}{14}
\bibcite{cuda-convnet}{15}
\bibcite{caffe-website}{16}
\bibcite{nips-tut}{17}
