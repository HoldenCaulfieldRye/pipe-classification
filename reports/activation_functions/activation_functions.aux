\relax 
\citation{krizhevsky}
\citation{DL-book}
\@writefile{toc}{\contentsline {section}{\numberline {1}Models of Neurons}{2}}
\@writefile{toc}{\contentsline {paragraph}{Binary Threshold Neuron}{2}}
\@writefile{toc}{\contentsline {paragraph}{Logistic Sigmoid Neuron}{2}}
\newlabel{sigmoid neuron}{{2}{2}}
\@writefile{toc}{\contentsline {subparagraph}{Logistic Unit as a Feature Detector}{2}}
\citation{krizhevsky}
\citation{rectifier}
\citation{krizhevsky}
\@writefile{toc}{\contentsline {paragraph}{Rectified Linear Neuron}{3}}
\newlabel{relu}{{3}{3}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Single-Input Rectified Linear Neuron\relax }}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Training: Backpropagation}{3}}
\bibcite{decaf}{1}
\bibcite{MIML}{2}
\@writefile{toc}{\contentsline {paragraph}{How the Gradient Propagates}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces (CHANGE NOTATION to reflect one above with $\textbf  {y}_L$ for top layer) Feed-Forward Neural Network with one Hidden Layer\relax }}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.0.1}Update Weight Values (with Stochastic Gradient Descent)}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.0.2}ReLUs Train Faster}{4}}
\bibcite{f-measure}{3}
\bibcite{control-point}{4}
\bibcite{univ-approx}{5}
\bibcite{DL-book}{6}
\bibcite{Russel & Norvig}{7}
\bibcite{krizhevsky}{8}
\bibcite{rectifier}{9}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces an error surface with poor local minima\relax }}{5}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces an error surface with poor local minima\relax }}{5}}
\bibcite{MLP-univ-approx}{10}
\bibcite{office}{11}
\bibcite{surf}{12}
\bibcite{transfer-learning}{13}
\bibcite{f-measure}{14}
\bibcite{cuda-convnet}{15}
