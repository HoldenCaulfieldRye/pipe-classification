\relax 
\citation{krizhevsky}
\citation{DL-book}
\citation{krizhevsky}
\citation{rectifier}
\@writefile{toc}{\contentsline {section}{\numberline {1}Models of Neurons}{2}}
\@writefile{toc}{\contentsline {paragraph}{Binary Threshold Neuron}{2}}
\@writefile{toc}{\contentsline {paragraph}{Logistic Sigmoid Neuron}{2}}
\newlabel{sigmoid neuron}{{2}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces single-input logistic sigmoid neuron}}{2}}
\@writefile{toc}{\contentsline {paragraph}{Rectified Linear Neuron}{2}}
\newlabel{relu}{{3}{2}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces single-input rectified linear neuron}}{3}}
\@writefile{toc}{\contentsline {paragraph}{Softmax Neuron}{3}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Training: Backpropagation}{3}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.0.1}Compute Error-Weight Partial Derivatives}{3}}
\bibcite{decaf}{1}
\bibcite{MIML}{2}
\@writefile{toc}{\contentsline {paragraph}{How Gradient Propagates}{4}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces (CHANGE NOTATION to reflect one above with $\textbf  {y}_L$ for top layer) Feed-Forward Neural Network with one Hidden Layer}}{4}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {2.0.2}Update Weight Values (with Stochastic Gradient Descent)}{4}}
\bibcite{f-measure}{3}
\bibcite{control-point}{4}
\bibcite{univ-approx}{5}
\bibcite{DL-book}{6}
\bibcite{Russel & Norvig}{7}
\bibcite{krizhevsky}{8}
\bibcite{rectifier}{9}
\bibcite{MLP-univ-approx}{10}
\bibcite{office}{11}
\bibcite{surf}{12}
\bibcite{transfer-learning}{13}
\bibcite{f-measure}{14}
\bibcite{cuda-convnet}{15}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces an error surface with poor local minima}}{5}}
