#!ini
[DEFAULT]
data-provider = basic-leaf256
include = $HERE/../defaults.cfg

[train]
layer-def = $HERE/layers.cfg
layer-params = $HERE/params.cfg
# need redbox and bluebox data in 1 dir!
data-path = /data2/ad6813/pipe-data/Bluebox/batches/scraping_peeling/net_0
train-range = 1-27
test-range = 28-33
save-path = $HERE/saves
give-up-epochs = 1000
test-freq = 17
test-many = -1
test-one = 0
f = /data2/ad6813/my-nets/saves/ConvNet__2014-07-08_14.54.04

[test]
layer-def = $HERE/layers.cfg
layer-params = $HERE/params.cfg
data-path = /data2/ad6813/pipe-data/test_run/batches 
test-range = 7245-9500
f = /data2/ad6813/my-nets/saves/ConvNet__2014-07-08_14.54.04
test-only=1

[show]
test-range = 9500-9661

[run]
f = $HERE/SavedWeights

[dataset]
# batching script takes labels based on dirname. so need a dir of raw
# data for each classification task. inside such a dir, need subdirs
# for each label.
# then, need an output path for the batches. the batches contain the
# labels, so if these batches only contain clamp labels, will need
# different batch sets for each task. that's inefficient. when you
# come back to this, need to redo batching, with a dict of labels, each
# key corresponds to a different task, and during training, can specify
# which labels are used to train. 
input-path = /data2/ad6813/pipe-data/Bluebox/raw_data/scraping_peeling/net_0
pattern = *.jpg
# collector = $HERE/../../../noccn/noccn/multitag_collector.py
output-path = /data2/ad6813/pipe-data/Bluebox/batches/scraping_peeling/net_0
batch-size = 128
size = (256, 256)
min_size = (128, 128)
transform_type = fitted
channels = 3
# MongoDB Query Information
# xml_query = 0
class_image_thres = 2000
# Note this defaults to None, and 0.0
# limit_by_component = Leaf
# component_prob_thres = 0.0
