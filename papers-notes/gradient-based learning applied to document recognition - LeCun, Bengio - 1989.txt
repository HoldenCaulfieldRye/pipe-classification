GRAPH TRANSFORMER NETWORK

overfit: Etest - Etrain = k*((h/P)^a)
P: no training samples
h: effective capacity
a, k are constants

local minima do not seem to be a problem for multi-layer neural networks
(or maybe just convolutional?): theoretical mystery.
conjecture: if network is oversized for the task, presence of extra
dimensions in parameter space reduces risk of unattainable regions.

SEGMENTATION: Heuristic Over-Segmentation.
generate a large number of potential cuts between characters using
heuristic image processing techniques, then select the best combinations
of cuts based on scores given for each candidate character by the recogniser.
major challenge: creating a labeled database of incorrectly segmented
characters to train the recogniser.

sol 1: run images of character strings through the segmenter
and manually label the character hypotheses.
difficult: eg should right half of a '4' be labeled as a 1 or as a
non character?

sol 2: train the system at the level of whole strings of chars, rather
than at the char level.
the system is trained to minimise an overall loss function which
measures the prob of an erroneous answer. cf section V to ensure that
the loss function is differentiable (hence can be used for gradient
descent).

sol 3: eliminate segmentation.
1) sweep the recogniser over every
possible location on the input image, rely on the character spotting
ppty of the recogniser: ie its ability to correctly recognise a well-
centred character in its input field, even in the presence of other
characters beside it. reject images containing no centered characters.
2) feed sequence of recogniser outputs to a GTN that takes linguistic
constraints into account and extracts the most likely interpretation.
Similar to hidden markov model.



GLOBALLY TRAINABLE SYSTEMS
what it is NOT: mulitple modules, eg for doc recog:
- field locator (extract regions of interest)
- field segmenter (cuts image into candidate chars)
- recogniser (classifies each candidate char)
- contextual post-processor (stochastic grammar: select best
grammatically correct answer from hypotheses generated by recog)

typically, each modules is optimised in isolation, then complete
system is assembled, and a subset of the parameters of the modules is
manually adjusted to maximise overall performance: that last part is
tedious, time consuming, probably suboptimal.

CASCADE OF MODULES:
better alternative: train entire system to minimise a global error
measure eg prob of character misclassification at the doc level.
because each module feeds into another, we can use backpropagation to
train the entire thing globally!

